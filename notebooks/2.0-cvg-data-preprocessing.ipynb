{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img width=210 height=150 src=\"../reports/figures/cunef_bw.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h1>2.0 Data Preprocessing </h1>\n",
    "Fake News Classifier <br>\n",
    "<strong>Ciencia de Datos para la Información No Estructurada</strong> <br>\n",
    "<strong>Master Universidatio en Ciencia de Datos</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align:right'>Carlos Viñals Guitart (<i>carlos.vinals@cunef.edu</i>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook nos centramos en elaborar el modelo que utilizaremos para detectar las noticias verdaderas y falsas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "X_train = pd.read_csv(\"../data/interim/train.csv\")\n",
    "Y_train = X_train['label']\n",
    "X_train = X_train['title']\n",
    "\n",
    "# Test data\n",
    "X_test = pd.read_csv(\"../data/interim/test.csv\")\n",
    "Y_test = X_test['label']\n",
    "X_test = X_test['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesado de los datos\n",
    "Realizaremos la limpieza de los datos, seguiremos el mismo procedimiento que en el análisis realizado. Crearemos una función que realizará la limpieza de los datos para cada fila del dataset, además incorporaremos la lematización con el objetivo de reducir la variabilidad de las palabras, verbos especialmente que permitirán manejar un menor vocabulario al modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translator to clean data \n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Word lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_cleaner(tlt):\n",
    "    \"\"\"\n",
    "    Cleans the text passed. All text to lower, blanck spaces and punctuation.\n",
    "    Also eliminates stopwords and lematize verbs\n",
    "    :param tlt: text to clean\n",
    "    :type tlt: string\n",
    "    \"\"\"\n",
    "    # Lower letters and blank spaces out, we eliminate all type of punctuation\n",
    "    # Finally we split the data (tokenize)\n",
    "    tokens = re.sub(r\"[^a-zA-Z0-9]\", \" \",tlt.strip().lower().translate(translator)).split()\n",
    "\n",
    "    # Lemmatize and eliminate any kind of stopword that doesn't gives us any information\n",
    "    final_words = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in tokens if not word in stopwords]\n",
    "\n",
    "    # Joining the resulted words cleaned\n",
    "    return \" \".join(final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la función para limpiar los datos (tanto en train como en test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.map(lambda x : title_cleaner(x))\n",
    "X_test = X_test.map(lambda x : title_cleaner(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora codificamos la variable objetivo mediante LabelEncoding, es decir, 0 - FAKE y 1 - REAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAKE' 'REAL']\n"
     ]
    }
   ],
   "source": [
    "lb = preprocessing.LabelEncoder()\n",
    "Y_train = lb.fit_transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)\n",
    "\n",
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de variables del modelo\n",
    "Ahora procedemos a crear las variables con los datos que serán utilizadas para el clasificador de noticias. Utilizaremos la técnica del Bag of Words y también la combinaremos con la técnica del TF-IDF. Crearemos muestras con solo el BoW y otra con el TF-IDF para poder utilizar cada una según el modelo y como consideremos oportuno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for creating variables\n",
    "corpus = X_train\n",
    "cv = CountVectorizer()\n",
    "tft = TfidfTransformer()\n",
    "\n",
    "# Creating the train result \n",
    "cv_matrix = cv.fit_transform(corpus)\n",
    "tfd_result = tft.fit_transform(cv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing train data\n",
    "# BoW only\n",
    "features_cv= cv.get_feature_names_out()\n",
    "X_train_bow = pd.DataFrame(cv_matrix.toarray(), columns=features_cv)\n",
    "\n",
    "# Tfd added\n",
    "features_tft = tft.get_feature_names_out()\n",
    "X_train_tfd = pd.DataFrame(tfd_result.toarray(), columns=features_tft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing test data\n",
    "# BoW only\n",
    "data_matrix = cv.transform(X_test) \n",
    "X_test_bow = pd.DataFrame(data_matrix.toarray(), columns=features_cv)\n",
    "\n",
    "# Tfd added\n",
    "X_test_tfd = pd.DataFrame(tft.transform(data_matrix).toarray(), columns=features_tft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardado de los datos\n",
    "Una vez procesados guardamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"train_bow.csv\", \"train_tfd.csv\", \"test_bow.csv\", \"test_tfd.csv\"]\n",
    "samples = [X_train_bow, X_train_tfd, X_test_bow, X_test_tfd]\n",
    "targets = [Y_train, Y_train, Y_test, Y_test]\n",
    "\n",
    "for i in range(4):\n",
    "    sample = samples[i]\n",
    "    sample['label'] = targets[i]\n",
    "    sample.to_csv(\"../data/processed/\" + filenames[i])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align:center'> Elaborado por Carlos Viñals Guitart (<i>carlos.vinals@cunef.edu</i>)</div>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe15c07fa5c976c9b177f61f867d39e76dfdb4dfd854f954709a4d876dae2d48"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('NSD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
