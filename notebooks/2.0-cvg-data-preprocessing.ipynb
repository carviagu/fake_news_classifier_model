{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img width=210 height=150 src=\"../reports/figures/cunef_bw.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<h1>2.0 Data Preprocessing </h1>\n",
    "Fake News Classifier <br>\n",
    "<strong>Ciencia de Datos para la Información No Estructurada</strong> <br>\n",
    "<strong>Master Universidatio en Ciencia de Datos</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align:right'>Carlos Viñals Guitart (<i>carlos.vinals@cunef.edu</i>)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook nos centramos en elaborar el modelo que utilizaremos para detectar las noticias verdaderas y falsas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\carviagu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data\n",
    "X_train = pd.read_csv(\"../data/interim/train.csv\")\n",
    "Y_train = X_train['label']\n",
    "X_train = X_train['title']\n",
    "\n",
    "# Test data\n",
    "X_test = pd.read_csv(\"../data/interim/test.csv\")\n",
    "Y_test = X_test['label']\n",
    "X_test = X_test['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesado de los datos\n",
    "Realizaremos la limpieza de los datos, seguiremos el mismo procedimiento que en el análisis realizado. Crearemos una función que realizará la limpieza de los datos para cada fila del dataset, además incorporaremos la lematización con el objetivo de reducir la variabilidad de las palabras, verbos especialmente que permitirán manejar un menor vocabulario al modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translator to clean data \n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Word lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_cleaner(tlt):\n",
    "    \"\"\"\n",
    "    Cleans the text passed. All text to lower, blanck spaces and punctuation.\n",
    "    Also eliminates stopwords and lematize verbs\n",
    "    :param tlt: text to clean\n",
    "    :type tlt: string\n",
    "    \"\"\"\n",
    "    # Lower letters and blank spaces out, we eliminate all type of punctuation\n",
    "    # Finally we split the data (tokenize)\n",
    "    tokens = re.sub(r\"[^a-zA-Z0-9]\", \" \",tlt.strip().lower().translate(translator)).split()\n",
    "\n",
    "    # Lemmatize and eliminate any kind of stopword that doesn't gives us any information\n",
    "    final_words = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in tokens if not word in stopwords]\n",
    "\n",
    "    # Joining the resulted words cleaned\n",
    "    return \" \".join(final_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la función para limpiar los datos (tanto en train como en test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.map(lambda x : title_cleaner(x))\n",
    "X_test = X_test.map(lambda x : title_cleaner(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora codificamos la variable objetivo mediante LabelEncoding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FAKE' 'REAL']\n"
     ]
    }
   ],
   "source": [
    "lb = preprocessing.LabelEncoder()\n",
    "Y_train = lb.fit_transform(Y_train)\n",
    "Y_test = lb.transform(Y_test)\n",
    "\n",
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación de variables del modelo\n",
    "Ahora procedemos a crear las variables con los datos que serán utilizadas para el clasificador de noticias. Utilizaremos la técnica del Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for creating variables\n",
    "corpus = X_train\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Creating the train result \n",
    "cv_matrix = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing train data\n",
    "features = cv.get_feature_names_out()\n",
    "data = cv_matrix.toarray()\n",
    "X_train = pd.DataFrame(data, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing test data\n",
    "features = cv.get_feature_names_out()\n",
    "data = cv.transform(X_test).toarray()\n",
    "X_test = pd.DataFrame(data, columns=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardado de los datos\n",
    "Una vez procesados guardamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['label'] = Y_train\n",
    "X_train.to_csv(\"../data/processed/train.csv\")\n",
    "\n",
    "X_test['label'] = Y_test\n",
    "X_test.to_csv(\"../data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align:center'> Elaborado por Carlos Viñals Guitart (<i>carlos.vinals@cunef.edu</i>)</div>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe15c07fa5c976c9b177f61f867d39e76dfdb4dfd854f954709a4d876dae2d48"
  },
  "kernelspec": {
   "display_name": "Python 3.9.11 ('NSD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
